{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: https://nbviewer.jupyter.org/github/rare-technologies/gensim/blob/develop/docs/notebooks/atmodel_tutorial.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2018-07-26 12:14:30--  http://www.cs.nyu.edu/~roweis/data/nips12raw_str602.tgz\n",
      "Resolving www.cs.nyu.edu (www.cs.nyu.edu)... 128.122.49.30\n",
      "Connecting to www.cs.nyu.edu (www.cs.nyu.edu)|128.122.49.30|:80... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: https://cs.nyu.edu/~roweis/data/nips12raw_str602.tgz [following]\n",
      "--2018-07-26 12:14:33--  https://cs.nyu.edu/~roweis/data/nips12raw_str602.tgz\n",
      "Resolving cs.nyu.edu (cs.nyu.edu)... 128.122.49.30\n",
      "Connecting to cs.nyu.edu (cs.nyu.edu)|128.122.49.30|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 12851423 (12M) [application/x-gzip]\n",
      "Saving to: ‘STDOUT’\n",
      "\n",
      "-                   100%[===================>]  12.26M   909KB/s    in 20s     \n",
      "\n",
      "2018-07-26 12:14:55 (616 KB/s) - written to stdout [12851423/12851423]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download NIPS data\n",
    "!wget -O - 'http://www.cs.nyu.edu/~roweis/data/nips12raw_str602.tgz' > nips12raw_str602.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unzip dataset, which is nicely annotated with authors\n",
    "import tarfile\n",
    "\n",
    "filename = 'nips12raw_str602.tgz'\n",
    "tar = tarfile.open(filename, 'r:gz')\n",
    "for item in tar:\n",
    "    tar.extract(item, path='.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re\n",
    "from smart_open import smart_open\n",
    "\n",
    "# Folder containing all NIPS papers.\n",
    "data_dir = './nipstxt/'  # Set this path to the data on your machine.\n",
    "\n",
    "# Folders containin individual NIPS papers.\n",
    "yrs = ['00', '01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12']\n",
    "dirs = ['nips' + yr for yr in yrs]\n",
    "\n",
    "# Get all document texts and their corresponding IDs.\n",
    "docs = []\n",
    "doc_ids = []\n",
    "for yr_dir in dirs:\n",
    "    files = os.listdir(data_dir + yr_dir)  # List of filenames.\n",
    "    for filen in files:\n",
    "        # Get document ID.\n",
    "        (idx1, idx2) = re.search('[0-9]+', filen).span()  # Matches the indexes of the start end end of the ID.\n",
    "        doc_ids.append(yr_dir[4:] + '_' + str(int(filen[idx1:idx2])))\n",
    "        \n",
    "        # Read document text.\n",
    "        # Note: ignoring characters that cause encoding errors.\n",
    "        with smart_open(data_dir + yr_dir + '/' + filen, 'rb', encoding='utf-8', errors='ignore' ) as fid:\n",
    "            txt = fid.read()\n",
    "            \n",
    "        # Replace any whitespace (newline, tabs, etc.) by a single space.\n",
    "        txt = re.sub('\\s', ' ', txt)\n",
    "        \n",
    "        docs.append(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1740"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smart_open import smart_open\n",
    "# filenames = [data_dir + 'idx/a' + yr + '.txt' for yr in yrs]  # Using the years defined in previous cell.\n",
    "\n",
    "# Get all author names and their corresponding document IDs.\n",
    "author2doc = dict()\n",
    "i = 0\n",
    "for yr in yrs:\n",
    "    # The files \"a00.txt\" and so on contain the author-document mappings.\n",
    "    filename = data_dir + 'idx/a' + yr + '.txt'\n",
    "    for line in smart_open(filename, 'rb', errors='ignore', encoding='utf-8'):\n",
    "        # Each line corresponds to one author.\n",
    "        contents = re.split(',', line)\n",
    "        author_name = (contents[1] + contents[0]).strip()\n",
    "        # Remove any whitespace to reduce redundant author names.\n",
    "        author_name = re.sub('\\s', '', author_name)\n",
    "        # Get document IDs for author.\n",
    "        ids = [c.strip() for c in contents[2:]]\n",
    "        if not author2doc.get(author_name):\n",
    "            # This is a new author.\n",
    "            author2doc[author_name] = []\n",
    "            i += 1\n",
    "        \n",
    "        # Add document IDs to author.\n",
    "        author2doc[author_name].extend([yr + '_' + id for id in ids])\n",
    "\n",
    "# Use an integer ID in author2doc, instead of the IDs provided in the NIPS dataset.\n",
    "# Mapping from ID of document in NIPS datast, to an integer ID.\n",
    "doc_id_dict = dict(zip(doc_ids, range(len(doc_ids))))\n",
    "# Replace NIPS IDs by integer IDs.\n",
    "for a, a_doc_ids in author2doc.items():\n",
    "    for i, doc_id in enumerate(a_doc_ids):\n",
    "        author2doc[a][i] = doc_id_dict[doc_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 28min 1s, sys: 7min 28s, total: 35min 29s\n",
      "Wall time: 20min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "processed_docs = []    \n",
    "for doc in nlp.pipe(docs, n_threads=4, batch_size=100):\n",
    "    # Process document using Spacy NLP pipeline.\n",
    "    \n",
    "    ents = doc.ents  # Named entities.\n",
    "\n",
    "    # Keep only words (no numbers, no punctuation).\n",
    "    # Lemmatize tokens, remove punctuation and remove stopwords.\n",
    "    doc = [token.lemma_ for token in doc if token.is_alpha and not token.is_stop]\n",
    "\n",
    "    # Remove common words from a stopword list.\n",
    "    #doc = [token for token in doc if token not in STOPWORDS]\n",
    "\n",
    "    # Add named entities, but only if they are a compound of more than word.\n",
    "    doc.extend([str(entity) for entity in ents if len(entity) > 1])\n",
    "    \n",
    "    processed_docs.append(doc)\n",
    "    \n",
    "# This method takes very long ~28 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = processed_docs\n",
    "del processed_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nus/git/tpms/venv/lib/python2.7/site-packages/gensim/models/phrases.py:494: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    }
   ],
   "source": [
    "# Compute bigrams.\n",
    "from gensim.models import Phrases\n",
    "# Add bigrams and trigrams to docs (only ones that appear 20 times or more).\n",
    "bigram = Phrases(docs, min_count=20)\n",
    "for idx in range(len(docs)):\n",
    "    for token in bigram[docs[idx]]:\n",
    "        if '_' in token:\n",
    "            # Token is a bigram, add to document.\n",
    "            docs[idx].append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary representation of the documents, and filter out frequent and rare words.\n",
    "\n",
    "from gensim.corpora import Dictionary\n",
    "dictionary = Dictionary(docs)\n",
    "\n",
    "# Remove rare and common tokens.\n",
    "# Filter out words that occur too frequently or too rarely.\n",
    "max_freq = 0.5\n",
    "min_wordcount = 20\n",
    "dictionary.filter_extremes(no_below=min_wordcount, no_above=max_freq)\n",
    "\n",
    "_ = dictionary[0]  # This sort of \"initializes\" dictionary.id2token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize data.\n",
    "\n",
    "# Bag-of-words representation of the documents.\n",
    "corpus = [dictionary.doc2bow(doc) for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of authors: 2479\n",
      "Number of unique tokens: 7478\n",
      "Number of documents: 1740\n"
     ]
    }
   ],
   "source": [
    "print('Number of authors: %d' % len(author2doc))\n",
    "print('Number of unique tokens: %d' % len(dictionary))\n",
    "print('Number of documents: %d' % len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.11 s, sys: 80.8 ms, total: 2.19 s\n",
      "Wall time: 2.2 s\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import AuthorTopicModel\n",
    "%time model = AuthorTopicModel(corpus=corpus, num_topics=10, id2word=dictionary.id2token, \\\n",
    "                author2doc=author2doc, chunksize=2000, passes=1, eval_every=0, \\\n",
    "                iterations=1, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min, sys: 6.72 s, total: 5min 7s\n",
      "Wall time: 5min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_list = []\n",
    "for i in range(5):\n",
    "    model = AuthorTopicModel(corpus=corpus, num_topics=10, id2word=dictionary.id2token, \\\n",
    "                    author2doc=author2doc, chunksize=2000, passes=100, gamma_threshold=1e-10, \\\n",
    "                    eval_every=0, iterations=1, random_state=i)\n",
    "    top_topics = model.top_topics(corpus)\n",
    "    tc = sum([t[1] for t in top_topics])\n",
    "    model_list.append((model, tc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic coherence: -1.089e+01\n"
     ]
    }
   ],
   "source": [
    "model, tc = max(model_list, key=lambda x: x[1])\n",
    "print('Topic coherence: %.3e' %tc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model.\n",
    "model.save('model.atmodel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model.\n",
    "model = AuthorTopicModel.load('model.atmodel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  u'0.013*\"signal\" + 0.010*\"control\" + 0.008*\"response\" + 0.008*\"movement\" + 0.008*\"field\" + 0.008*\"visual\" + 0.008*\"motor\" + 0.007*\"filter\" + 0.007*\"position\" + 0.006*\"target\"'),\n",
       " (1,\n",
       "  u'0.008*\"neuron\" + 0.008*\"f\" + 0.008*\"memory\" + 0.008*\"matrix\" + 0.007*\"w\" + 0.007*\"threshold\" + 0.006*\"let\" + 0.005*\"capacity\" + 0.005*\"layer\" + 0.005*\"g\"'),\n",
       " (2,\n",
       "  u'0.010*\"gaussian\" + 0.008*\"mixture\" + 0.007*\"density\" + 0.007*\"likelihood\" + 0.006*\"prior\" + 0.006*\"sample\" + 0.006*\"data\" + 0.006*\"matrix\" + 0.006*\"noise\" + 0.005*\"component\"'),\n",
       " (3,\n",
       "  u'0.009*\"generalization\" + 0.009*\"rule\" + 0.007*\"bound\" + 0.006*\"f\" + 0.005*\"w\" + 0.005*\"finite\" + 0.005*\"sequence\" + 0.005*\"let\" + 0.004*\"theorem\" + 0.004*\"optimal\"'),\n",
       " (4,\n",
       "  u'0.030*\"neuron\" + 0.014*\"spike\" + 0.012*\"cell\" + 0.010*\"synaptic\" + 0.008*\"frequency\" + 0.008*\"response\" + 0.007*\"firing\" + 0.007*\"potential\" + 0.006*\"delay\" + 0.006*\"circuit\"'),\n",
       " (5,\n",
       "  u'0.021*\"image\" + 0.009*\"representation\" + 0.006*\"face\" + 0.006*\"object\" + 0.006*\"recognition\" + 0.006*\"hidden\" + 0.005*\"layer\" + 0.005*\"classification\" + 0.005*\"view\" + 0.004*\"human\"'),\n",
       " (6,\n",
       "  u'0.022*\"cell\" + 0.012*\"visual\" + 0.009*\"map\" + 0.009*\"activity\" + 0.009*\"orientation\" + 0.008*\"direction\" + 0.008*\"stimulus\" + 0.008*\"object\" + 0.008*\"field\" + 0.007*\"motion\"'),\n",
       " (7,\n",
       "  u'0.015*\"recognition\" + 0.012*\"speech\" + 0.011*\"word\" + 0.010*\"layer\" + 0.008*\"net\" + 0.006*\"sequence\" + 0.006*\"character\" + 0.005*\"architecture\" + 0.005*\"class\" + 0.005*\"context\"'),\n",
       " (8,\n",
       "  u'0.013*\"chip\" + 0.011*\"analog\" + 0.011*\"circuit\" + 0.008*\"signal\" + 0.007*\"implementation\" + 0.006*\"design\" + 0.006*\"vlsi\" + 0.006*\"implement\" + 0.005*\"node\" + 0.005*\"voltage\"'),\n",
       " (9,\n",
       "  u'0.010*\"control\" + 0.009*\"action\" + 0.008*\"policy\" + 0.008*\"decision\" + 0.007*\"classifier\" + 0.007*\"optimal\" + 0.007*\"q\" + 0.007*\"reinforcement\" + 0.005*\"cost\" + 0.004*\"reinforcement_learning\"')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.show_topics(num_topics=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
