{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.split(os.getcwd())[0])\n",
    "import utils\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../dataset/p0.txt',\n",
       " '../dataset/p1.txt',\n",
       " '../dataset/p2.txt',\n",
       " '../dataset/p3.txt',\n",
       " '../dataset/p4.txt',\n",
       " '../dataset/p5.txt',\n",
       " '../dataset/p6.txt',\n",
       " '../dataset/p7.txt',\n",
       " '../dataset/p8.txt',\n",
       " '../dataset/p9.txt',\n",
       " '../dataset/p10.txt',\n",
       " '../dataset/p11.txt',\n",
       " '../dataset/p12.txt',\n",
       " '../dataset/p13.txt',\n",
       " '../dataset/p14.txt',\n",
       " '../dataset/p15.txt',\n",
       " '../dataset/p16.txt',\n",
       " '../dataset/p17.txt',\n",
       " '../dataset/p18.txt',\n",
       " '../dataset/p19.txt',\n",
       " '../dataset/p20.txt',\n",
       " '../dataset/p21.txt',\n",
       " '../dataset/p22.txt',\n",
       " '../dataset/p23.txt',\n",
       " '../dataset/p24.txt',\n",
       " '../dataset/p25.txt',\n",
       " '../dataset/p26.txt',\n",
       " '../dataset/p27.txt',\n",
       " '../dataset/p28.txt',\n",
       " '../dataset/p29.txt',\n",
       " '../dataset/p30.txt',\n",
       " '../dataset/p31.txt',\n",
       " '../dataset/p32.txt',\n",
       " '../dataset/p33.txt',\n",
       " '../dataset/p34.txt',\n",
       " '../dataset/p35.txt',\n",
       " '../dataset/p36.txt',\n",
       " '../dataset/p37.txt',\n",
       " '../dataset/p38.txt',\n",
       " '../dataset/p39.txt',\n",
       " '../dataset/p40.txt',\n",
       " '../dataset/p41.txt',\n",
       " '../dataset/p42.txt',\n",
       " '../dataset/p43.txt',\n",
       " '../dataset/p44.txt',\n",
       " '../dataset/p45.txt',\n",
       " '../dataset/p46.txt',\n",
       " '../dataset/p47.txt',\n",
       " '../dataset/p48.txt',\n",
       " '../dataset/p49.txt',\n",
       " '../dataset/p50.txt',\n",
       " '../dataset/p51.txt',\n",
       " '../dataset/p52.txt',\n",
       " '../dataset/p53.txt',\n",
       " '../dataset/p54.txt',\n",
       " '../dataset/p55.txt',\n",
       " '../dataset/p56.txt',\n",
       " '../dataset/p57.txt',\n",
       " '../dataset/p58.txt',\n",
       " '../dataset/p59.txt',\n",
       " '../dataset/p60.txt',\n",
       " '../dataset/p61.txt',\n",
       " '../dataset/p62.txt',\n",
       " '../dataset/p63.txt',\n",
       " '../dataset/p64.txt',\n",
       " '../dataset/p65.txt',\n",
       " '../dataset/p66.txt',\n",
       " '../dataset/p67.txt',\n",
       " '../dataset/p68.txt',\n",
       " '../dataset/p69.txt',\n",
       " '../dataset/p70.txt',\n",
       " '../dataset/p71.txt',\n",
       " '../dataset/p72.txt',\n",
       " '../dataset/p73.txt',\n",
       " '../dataset/p74.txt',\n",
       " '../dataset/p75.txt',\n",
       " '../dataset/p76.txt',\n",
       " '../dataset/p77.txt',\n",
       " '../dataset/p78.txt',\n",
       " '../dataset/p79.txt',\n",
       " '../dataset/p80.txt',\n",
       " '../dataset/p81.txt',\n",
       " '../dataset/p82.txt',\n",
       " '../dataset/p83.txt',\n",
       " '../dataset/p84.txt',\n",
       " '../dataset/p85.txt',\n",
       " '../dataset/p86.txt',\n",
       " '../dataset/p87.txt',\n",
       " '../dataset/p88.txt',\n",
       " '../dataset/p89.txt',\n",
       " '../dataset/p90.txt',\n",
       " '../dataset/p91.txt',\n",
       " '../dataset/p92.txt',\n",
       " '../dataset/p93.txt',\n",
       " '../dataset/p94.txt',\n",
       " '../dataset/p95.txt',\n",
       " '../dataset/p96.txt',\n",
       " '../dataset/p97.txt',\n",
       " '../dataset/p98.txt',\n",
       " '../dataset/p99.txt',\n",
       " '../dataset/p100.txt',\n",
       " '../dataset/p101.txt',\n",
       " '../dataset/p102.txt',\n",
       " '../dataset/p103.txt',\n",
       " '../dataset/p104.txt',\n",
       " '../dataset/p105.txt',\n",
       " '../dataset/p106.txt',\n",
       " '../dataset/p107.txt',\n",
       " '../dataset/p108.txt',\n",
       " '../dataset/p109.txt',\n",
       " '../dataset/p110.txt',\n",
       " '../dataset/p111.txt',\n",
       " '../dataset/p112.txt',\n",
       " '../dataset/p113.txt',\n",
       " '../dataset/p114.txt',\n",
       " '../dataset/p115.txt',\n",
       " '../dataset/p116.txt',\n",
       " '../dataset/p117.txt',\n",
       " '../dataset/p118.txt',\n",
       " '../dataset/p119.txt',\n",
       " '../dataset/p120.txt',\n",
       " '../dataset/p121.txt',\n",
       " '../dataset/p122.txt',\n",
       " '../dataset/p123.txt',\n",
       " '../dataset/p124.txt',\n",
       " '../dataset/p125.txt',\n",
       " '../dataset/p126.txt',\n",
       " '../dataset/p127.txt',\n",
       " '../dataset/p128.txt',\n",
       " '../dataset/p129.txt',\n",
       " '../dataset/p130.txt',\n",
       " '../dataset/p131.txt',\n",
       " '../dataset/p132.txt',\n",
       " '../dataset/p133.txt',\n",
       " '../dataset/p134.txt',\n",
       " '../dataset/p135.txt',\n",
       " '../dataset/p136.txt',\n",
       " '../dataset/p137.txt',\n",
       " '../dataset/p138.txt',\n",
       " '../dataset/p139.txt',\n",
       " '../dataset/p140.txt',\n",
       " '../dataset/p141.txt',\n",
       " '../dataset/p142.txt',\n",
       " '../dataset/p143.txt',\n",
       " '../dataset/p144.txt',\n",
       " '../dataset/p145.txt',\n",
       " '../dataset/p146.txt',\n",
       " '../dataset/p147.txt',\n",
       " '../dataset/p148.txt',\n",
       " '../dataset/p149.txt',\n",
       " '../dataset/p150.txt',\n",
       " '../dataset/p151.txt',\n",
       " '../dataset/p152.txt',\n",
       " '../dataset/p153.txt',\n",
       " '../dataset/p154.txt',\n",
       " '../dataset/p155.txt',\n",
       " '../dataset/p156.txt',\n",
       " '../dataset/p157.txt',\n",
       " '../dataset/p158.txt',\n",
       " '../dataset/p159.txt',\n",
       " '../dataset/p160.txt',\n",
       " '../dataset/p161.txt',\n",
       " '../dataset/p162.txt',\n",
       " '../dataset/p163.txt',\n",
       " '../dataset/p164.txt',\n",
       " '../dataset/p165.txt',\n",
       " '../dataset/p166.txt',\n",
       " '../dataset/p167.txt',\n",
       " '../dataset/p168.txt',\n",
       " '../dataset/p169.txt',\n",
       " '../dataset/p170.txt',\n",
       " '../dataset/p171.txt',\n",
       " '../dataset/p172.txt',\n",
       " '../dataset/p173.txt',\n",
       " '../dataset/p174.txt',\n",
       " '../dataset/p175.txt',\n",
       " '../dataset/p176.txt',\n",
       " '../dataset/p177.txt',\n",
       " '../dataset/p178.txt',\n",
       " '../dataset/p179.txt',\n",
       " '../dataset/p180.txt',\n",
       " '../dataset/p181.txt',\n",
       " '../dataset/p182.txt',\n",
       " '../dataset/p183.txt',\n",
       " '../dataset/p184.txt',\n",
       " '../dataset/p185.txt',\n",
       " '../dataset/p186.txt',\n",
       " '../dataset/p187.txt',\n",
       " '../dataset/p188.txt',\n",
       " '../dataset/p189.txt',\n",
       " '../dataset/p190.txt',\n",
       " '../dataset/p191.txt',\n",
       " '../dataset/p192.txt',\n",
       " '../dataset/p193.txt',\n",
       " '../dataset/p194.txt',\n",
       " '../dataset/p195.txt',\n",
       " '../dataset/p196.txt',\n",
       " '../dataset/p197.txt',\n",
       " '../dataset/p198.txt',\n",
       " '../dataset/p199.txt',\n",
       " '../dataset/p200.txt',\n",
       " '../dataset/p201.txt',\n",
       " '../dataset/p202.txt',\n",
       " '../dataset/p203.txt',\n",
       " '../dataset/p204.txt',\n",
       " '../dataset/p205.txt',\n",
       " '../dataset/p206.txt',\n",
       " '../dataset/p207.txt',\n",
       " '../dataset/p208.txt',\n",
       " '../dataset/p209.txt',\n",
       " '../dataset/p210.txt',\n",
       " '../dataset/p211.txt',\n",
       " '../dataset/p212.txt',\n",
       " '../dataset/p213.txt',\n",
       " '../dataset/p214.txt',\n",
       " '../dataset/p215.txt',\n",
       " '../dataset/p216.txt',\n",
       " '../dataset/p217.txt',\n",
       " '../dataset/p218.txt',\n",
       " '../dataset/p219.txt',\n",
       " '../dataset/p220.txt',\n",
       " '../dataset/p221.txt',\n",
       " '../dataset/p222.txt',\n",
       " '../dataset/p223.txt',\n",
       " '../dataset/p224.txt',\n",
       " '../dataset/p225.txt',\n",
       " '../dataset/p226.txt',\n",
       " '../dataset/p227.txt',\n",
       " '../dataset/p228.txt',\n",
       " '../dataset/p229.txt',\n",
       " '../dataset/p230.txt',\n",
       " '../dataset/p231.txt',\n",
       " '../dataset/p232.txt',\n",
       " '../dataset/p233.txt',\n",
       " '../dataset/p234.txt',\n",
       " '../dataset/p235.txt',\n",
       " '../dataset/p236.txt',\n",
       " '../dataset/p237.txt',\n",
       " '../dataset/p238.txt',\n",
       " '../dataset/p239.txt',\n",
       " '../dataset/p240.txt',\n",
       " '../dataset/p241.txt',\n",
       " '../dataset/p242.txt',\n",
       " '../dataset/p243.txt',\n",
       " '../dataset/p244.txt',\n",
       " '../dataset/p245.txt',\n",
       " '../dataset/p246.txt',\n",
       " '../dataset/p247.txt',\n",
       " '../dataset/p248.txt',\n",
       " '../dataset/p249.txt',\n",
       " '../dataset/p250.txt',\n",
       " '../dataset/p251.txt',\n",
       " '../dataset/p252.txt',\n",
       " '../dataset/p253.txt',\n",
       " '../dataset/p254.txt',\n",
       " '../dataset/p255.txt',\n",
       " '../dataset/p256.txt',\n",
       " '../dataset/p257.txt',\n",
       " '../dataset/p258.txt',\n",
       " '../dataset/p259.txt',\n",
       " '../dataset/p260.txt',\n",
       " '../dataset/p261.txt',\n",
       " '../dataset/p262.txt',\n",
       " '../dataset/p263.txt',\n",
       " '../dataset/p264.txt',\n",
       " '../dataset/p265.txt',\n",
       " '../dataset/p266.txt',\n",
       " '../dataset/p267.txt',\n",
       " '../dataset/p268.txt',\n",
       " '../dataset/p269.txt',\n",
       " '../dataset/p270.txt',\n",
       " '../dataset/p271.txt',\n",
       " '../dataset/p272.txt',\n",
       " '../dataset/p273.txt',\n",
       " '../dataset/p274.txt',\n",
       " '../dataset/p275.txt',\n",
       " '../dataset/p276.txt',\n",
       " '../dataset/p277.txt',\n",
       " '../dataset/p278.txt',\n",
       " '../dataset/p279.txt',\n",
       " '../dataset/p280.txt',\n",
       " '../dataset/p281.txt',\n",
       " '../dataset/p282.txt',\n",
       " '../dataset/p283.txt',\n",
       " '../dataset/p284.txt',\n",
       " '../dataset/p285.txt',\n",
       " '../dataset/p286.txt',\n",
       " '../dataset/p287.txt',\n",
       " '../dataset/p288.txt',\n",
       " '../dataset/p289.txt',\n",
       " '../dataset/p290.txt',\n",
       " '../dataset/p291.txt',\n",
       " '../dataset/p292.txt',\n",
       " '../dataset/p293.txt',\n",
       " '../dataset/p294.txt',\n",
       " '../dataset/p295.txt',\n",
       " '../dataset/p296.txt',\n",
       " '../dataset/p297.txt',\n",
       " '../dataset/p298.txt',\n",
       " '../dataset/p299.txt',\n",
       " '../dataset/p300.txt',\n",
       " '../dataset/p301.txt',\n",
       " '../dataset/p302.txt',\n",
       " '../dataset/p303.txt',\n",
       " '../dataset/p304.txt',\n",
       " '../dataset/p305.txt',\n",
       " '../dataset/p306.txt',\n",
       " '../dataset/p307.txt',\n",
       " '../dataset/p308.txt',\n",
       " '../dataset/p309.txt',\n",
       " '../dataset/p310.txt',\n",
       " '../dataset/p311.txt',\n",
       " '../dataset/p312.txt',\n",
       " '../dataset/p313.txt',\n",
       " '../dataset/p314.txt',\n",
       " '../dataset/p315.txt',\n",
       " '../dataset/p316.txt',\n",
       " '../dataset/p317.txt',\n",
       " '../dataset/p318.txt']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def atoi(text):\n",
    "    return int(text) if text.isdigit() else text\n",
    "def natural_keys(text):\n",
    "    '''\n",
    "    alist.sort(key=natural_keys) sorts in human order\n",
    "    http://nedbatchelder.com/blog/200712/human_sorting.html\n",
    "    (See Toothy's implementation in the comments)\n",
    "    '''\n",
    "    return [ atoi(c) for c in re.split('(\\d+)', text) ]\n",
    "\n",
    "dataset_dir = \"../dataset/\"\n",
    "files = glob.glob(dataset_dir+\"p*.txt\")\n",
    "files.sort(key=natural_keys)\n",
    "docs = [utils.read_file(x) for x in files]\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11min 27s, sys: 2min 57s, total: 14min 24s\n",
      "Wall time: 8min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "processed_docs = []    \n",
    "for doc in nlp.pipe(docs, n_threads=4, batch_size=100):\n",
    "    # Process document using Spacy NLP pipeline.\n",
    "    \n",
    "    ents = doc.ents  # Named entities.\n",
    "\n",
    "    # Keep only words (no numbers, no punctuation).\n",
    "    # Lemmatize tokens, remove punctuation and remove stopwords.\n",
    "    doc = [token.lemma_ for token in doc if token.is_alpha and not token.is_stop]\n",
    "\n",
    "    # Remove common words from a stopword list.\n",
    "    #doc = [token for token in doc if token not in STOPWORDS]\n",
    "\n",
    "    # Add named entities, but only if they are a compound of more than word.\n",
    "    doc.extend([str(entity) for entity in ents if len(entity) > 1])\n",
    "    \n",
    "    processed_docs.append(doc)\n",
    "    \n",
    "# This method takes roughly ~ 10 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = processed_docs\n",
    "del processed_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "319"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nus/git/tpms/venv/lib/python2.7/site-packages/gensim/models/phrases.py:494: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    }
   ],
   "source": [
    "# Compute bigrams.\n",
    "from gensim.models import Phrases\n",
    "# Add bigrams and trigrams to docs (only ones that appear 20 times or more).\n",
    "bigram = Phrases(docs, min_count=20)\n",
    "for idx in range(len(docs)):\n",
    "    for token in bigram[docs[idx]]:\n",
    "        if '_' in token:\n",
    "            # Token is a bigram, add to document.\n",
    "            docs[idx].append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary representation of the documents, and filter out frequent and rare words.\n",
    "\n",
    "from gensim.corpora import Dictionary\n",
    "dictionary = Dictionary(docs)\n",
    "\n",
    "# Remove rare and common tokens.\n",
    "# Filter out words that occur too frequently or too rarely.\n",
    "max_freq = 0.5\n",
    "min_wordcount = 20\n",
    "dictionary.filter_extremes(no_below=min_wordcount, no_above=max_freq)\n",
    "\n",
    "_ = dictionary[0]  # This sort of \"initializes\" dictionary.id2token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize data.\n",
    "\n",
    "# Bag-of-words representation of the documents.\n",
    "corpus = [dictionary.doc2bow(doc) for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'Anthony_Tung': [84,\n",
       "  85,\n",
       "  86,\n",
       "  87,\n",
       "  88,\n",
       "  89,\n",
       "  90,\n",
       "  91,\n",
       "  92,\n",
       "  93,\n",
       "  94,\n",
       "  95,\n",
       "  96,\n",
       "  97,\n",
       "  98,\n",
       "  99,\n",
       "  100,\n",
       "  101,\n",
       "  102,\n",
       "  103],\n",
       " u'Beng_Chin_OOI': [194,\n",
       "  195,\n",
       "  196,\n",
       "  197,\n",
       "  198,\n",
       "  199,\n",
       "  200,\n",
       "  201,\n",
       "  202,\n",
       "  203,\n",
       "  204,\n",
       "  205,\n",
       "  165,\n",
       "  166,\n",
       "  206,\n",
       "  207,\n",
       "  208,\n",
       "  209,\n",
       "  210,\n",
       "  211,\n",
       "  212],\n",
       " u'Bingsheng_He': [47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58],\n",
       " u'Bryan_Low': [25,\n",
       "  26,\n",
       "  27,\n",
       "  28,\n",
       "  29,\n",
       "  30,\n",
       "  31,\n",
       "  32,\n",
       "  33,\n",
       "  34,\n",
       "  35,\n",
       "  36,\n",
       "  37,\n",
       "  38,\n",
       "  39,\n",
       "  40,\n",
       "  41,\n",
       "  42,\n",
       "  43,\n",
       "  44,\n",
       "  45,\n",
       "  46],\n",
       " u'David_Hsu': [117,\n",
       "  118,\n",
       "  119,\n",
       "  120,\n",
       "  121,\n",
       "  122,\n",
       "  123,\n",
       "  124,\n",
       "  125,\n",
       "  126,\n",
       "  127,\n",
       "  128,\n",
       "  129,\n",
       "  130,\n",
       "  131,\n",
       "  132,\n",
       "  133,\n",
       "  134,\n",
       "  135],\n",
       " u'David_S._Rosenblum': [213,\n",
       "  214,\n",
       "  215,\n",
       "  216,\n",
       "  217,\n",
       "  218,\n",
       "  219,\n",
       "  220,\n",
       "  221,\n",
       "  222,\n",
       "  174,\n",
       "  223],\n",
       " u'Gim_Hee_Lee': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],\n",
       " u'Haizhou_Li': [104,\n",
       "  105,\n",
       "  106,\n",
       "  107,\n",
       "  108,\n",
       "  109,\n",
       "  110,\n",
       "  111,\n",
       "  112,\n",
       "  113,\n",
       "  114,\n",
       "  115,\n",
       "  116],\n",
       " u'Harold_Soh': [312, 313, 314, 315, 316, 317, 318, 132, 133],\n",
       " u'Hwee_Tou_Ng': [235,\n",
       "  236,\n",
       "  237,\n",
       "  238,\n",
       "  239,\n",
       "  153,\n",
       "  240,\n",
       "  241,\n",
       "  242,\n",
       "  243,\n",
       "  244,\n",
       "  245,\n",
       "  246,\n",
       "  247,\n",
       "  248,\n",
       "  249,\n",
       "  158,\n",
       "  250,\n",
       "  251],\n",
       " u'Kay_Chen_Tan': [105, 158, 159, 108, 160, 115],\n",
       " u'Kuldeep_S._Meel': [252,\n",
       "  253,\n",
       "  254,\n",
       "  255,\n",
       "  256,\n",
       "  257,\n",
       "  258,\n",
       "  259,\n",
       "  260,\n",
       "  261,\n",
       "  262,\n",
       "  263,\n",
       "  264,\n",
       "  265,\n",
       "  266,\n",
       "  267,\n",
       "  268,\n",
       "  269,\n",
       "  270,\n",
       "  271,\n",
       "  272,\n",
       "  273],\n",
       " u'Lee_Wee_Sun': [149,\n",
       "  119,\n",
       "  120,\n",
       "  123,\n",
       "  150,\n",
       "  151,\n",
       "  152,\n",
       "  126,\n",
       "  153,\n",
       "  6,\n",
       "  154,\n",
       "  128,\n",
       "  129,\n",
       "  155,\n",
       "  131,\n",
       "  156,\n",
       "  157,\n",
       "  134,\n",
       "  135],\n",
       " u'Leong_Tze_Yun': [301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311],\n",
       " u'Min-Yen_Kan': [288,\n",
       "  289,\n",
       "  290,\n",
       "  291,\n",
       "  292,\n",
       "  293,\n",
       "  294,\n",
       "  295,\n",
       "  296,\n",
       "  297,\n",
       "  298,\n",
       "  299,\n",
       "  300],\n",
       " u'Mohan_S._Kankanhalli': [136,\n",
       "  137,\n",
       "  138,\n",
       "  139,\n",
       "  140,\n",
       "  141,\n",
       "  142,\n",
       "  143,\n",
       "  144,\n",
       "  145,\n",
       "  146,\n",
       "  147,\n",
       "  148],\n",
       " u'Rudy_Setiono': [59, 60, 61, 62, 63, 64, 65, 66],\n",
       " u'Sanjay_Jain': [13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24],\n",
       " u'Shuicheng_Yan': [67,\n",
       "  68,\n",
       "  69,\n",
       "  70,\n",
       "  71,\n",
       "  72,\n",
       "  73,\n",
       "  74,\n",
       "  75,\n",
       "  76,\n",
       "  77,\n",
       "  78,\n",
       "  79,\n",
       "  80,\n",
       "  81,\n",
       "  82,\n",
       "  83],\n",
       " u'Shuzhi_Sam_Ge': [166, 167, 168, 60, 169, 170, 171, 172],\n",
       " u'Tat_Seng_CHUA': [173,\n",
       "  174,\n",
       "  175,\n",
       "  176,\n",
       "  177,\n",
       "  178,\n",
       "  179,\n",
       "  180,\n",
       "  181,\n",
       "  182,\n",
       "  183,\n",
       "  184,\n",
       "  185,\n",
       "  186,\n",
       "  187,\n",
       "  188,\n",
       "  189,\n",
       "  190,\n",
       "  191,\n",
       "  192,\n",
       "  193],\n",
       " u'Teck_Khim_Ng': [161, 162, 163, 164, 165],\n",
       " u'Terence_Sim': [224, 225, 226, 227, 228, 76, 229, 230, 231, 232, 233, 234],\n",
       " u'Wee_Sun_Lee': [150,\n",
       "  119,\n",
       "  120,\n",
       "  122,\n",
       "  123,\n",
       "  151,\n",
       "  152,\n",
       "  153,\n",
       "  126,\n",
       "  154,\n",
       "  6,\n",
       "  155,\n",
       "  128,\n",
       "  129,\n",
       "  131,\n",
       "  157,\n",
       "  158,\n",
       "  134,\n",
       "  135],\n",
       " u'Wynne_Hsu': [274,\n",
       "  275,\n",
       "  276,\n",
       "  277,\n",
       "  278,\n",
       "  279,\n",
       "  280,\n",
       "  281,\n",
       "  282,\n",
       "  283,\n",
       "  284,\n",
       "  285,\n",
       "  174,\n",
       "  286,\n",
       "  287]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "author2doc = utils.read_json_file(\"../dataset/authors.json\")\n",
    "author2doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import AuthorTopicModel\n",
    "%time model = AuthorTopicModel(corpus=corpus, num_topics=10, id2word=dictionary.id2token, \\\n",
    "                author2doc=author2doc, chunksize=2000, passes=1, eval_every=0, \\\n",
    "                iterations=1, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 2s, sys: 2.37 s, total: 1min 4s\n",
      "Wall time: 1min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# model_list = []\n",
    "\n",
    "from gensim.models import AuthorTopicModel\n",
    "\n",
    "model_list = []\n",
    "num_topics = 15\n",
    "\n",
    "for i in range(5):\n",
    "    model = AuthorTopicModel(corpus=corpus, num_topics=num_topics, id2word=dictionary.id2token, \\\n",
    "                    author2doc=author2doc, chunksize=2000, passes=100, gamma_threshold=1e-10, \\\n",
    "                    eval_every=0, iterations=1, random_state=i)\n",
    "    top_topics = model.top_topics(corpus)\n",
    "    tc = sum([t[1] for t in top_topics])\n",
    "    model_list.append((model, tc))\n",
    "\n",
    "#     print(\"Perplexity = %.3e\" %model.bound(chunk=corpus, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic coherence: -1.557e+01\n"
     ]
    }
   ],
   "source": [
    "model, tc = max(model_list, key=lambda x: x[1])\n",
    "print('Topic coherence: %.3e' %tc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model.\n",
    "model.save('model.atmodel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  u'0.024*\"face\" + 0.023*\"image\" + 0.011*\"facial\" + 0.010*\"occlusion\" + 0.010*\"age\" + 0.009*\"layer\" + 0.007*\"object\" + 0.007*\"detection\" + 0.006*\"deep\" + 0.005*\"cnn\"'),\n",
       " (1,\n",
       "  u'0.021*\"image\" + 0.017*\"fault\" + 0.016*\"signal\" + 0.015*\"position\" + 0.014*\"light\" + 0.011*\"motion\" + 0.011*\"detection\" + 0.009*\"constraint\" + 0.008*\"pass\" + 0.008*\"robot\"'),\n",
       " (2,\n",
       "  u'0.010*\"graph\" + 0.009*\"node\" + 0.008*\"u\" + 0.008*\"gpu\" + 0.008*\"transaction\" + 0.008*\"query\" + 0.007*\"concept\" + 0.007*\"memory\" + 0.006*\"block\" + 0.006*\"record\"'),\n",
       " (3,\n",
       "  u'0.012*\"query\" + 0.012*\"image\" + 0.009*\"record\" + 0.008*\"index\" + 0.008*\"cluster\" + 0.007*\"attribute\" + 0.006*\"similarity\" + 0.006*\"edge\" + 0.006*\"selection\" + 0.005*\"tuple\"'),\n",
       " (4,\n",
       "  u'0.026*\"consumer\" + 0.021*\"image\" + 0.016*\"goal\" + 0.013*\"package\" + 0.010*\"simulation\" + 0.009*\"concept\" + 0.009*\"period\" + 0.009*\"product\" + 0.007*\"price\" + 0.007*\"self\"'),\n",
       " (5,\n",
       "  u'0.015*\"variable\" + 0.015*\"formula\" + 0.012*\"count\" + 0.010*\"sat\" + 0.010*\"q\" + 0.010*\"counting\" + 0.010*\"constraint\" + 0.010*\"benchmark\" + 0.009*\"independent\" + 0.008*\"rf\"'),\n",
       " (6,\n",
       "  u'0.015*\"gp\" + 0.011*\"gaussian\" + 0.010*\"agent\" + 0.009*\"q\" + 0.009*\"u\" + 0.008*\"proc\" + 0.007*\"in_proc\" + 0.007*\"dt\" + 0.007*\"z\" + 0.007*\"robot\"'),\n",
       " (7,\n",
       "  u'0.029*\"speech\" + 0.015*\"language\" + 0.014*\"pp\" + 0.009*\"tool\" + 0.008*\"signal\" + 0.008*\"alignment\" + 0.007*\"acoustic\" + 0.007*\"speaker\" + 0.007*\"gan\" + 0.007*\"deep\"'),\n",
       " (8,\n",
       "  u'0.016*\"language\" + 0.012*\"translation\" + 0.012*\"english\" + 0.011*\"rule\" + 0.011*\"in_proceeding\" + 0.010*\"score\" + 0.008*\"sentence\" + 0.008*\"relation\" + 0.008*\"workshop\" + 0.007*\"chinese\"'),\n",
       " (9,\n",
       "  u'0.020*\"decision\" + 0.018*\"action\" + 0.016*\"concept\" + 0.016*\"node\" + 0.013*\"variable\" + 0.011*\"policy\" + 0.010*\"transition\" + 0.009*\"selection\" + 0.008*\"reward\" + 0.008*\"relation\"'),\n",
       " (10,\n",
       "  u'0.008*\"social\" + 0.008*\"recommendation\" + 0.007*\"temporal\" + 0.007*\"activity\" + 0.006*\"attribute\" + 0.006*\"relation\" + 0.006*\"app\" + 0.005*\"node\" + 0.005*\"entity\" + 0.005*\"water\"'),\n",
       " (11,\n",
       "  u'0.016*\"trust\" + 0.012*\"kernel\" + 0.011*\"cluster\" + 0.010*\"pp\" + 0.010*\"fig\" + 0.010*\"object\" + 0.009*\"accuracy\" + 0.008*\"vol\" + 0.008*\"version\" + 0.006*\"gaussian\"'),\n",
       " (12,\n",
       "  u'0.024*\"image\" + 0.024*\"camera\" + 0.015*\"pose\" + 0.014*\"of\" + 0.012*\"motion\" + 0.009*\"and\" + 0.009*\"cloud\" + 0.009*\"robot\" + 0.008*\"vision\" + 0.008*\"depth\"'),\n",
       " (13,\n",
       "  u'0.014*\"image\" + 0.014*\"item\" + 0.010*\"video\" + 0.008*\"social\" + 0.008*\"ad\" + 0.008*\"rating\" + 0.008*\"recommendation\" + 0.008*\"u\" + 0.007*\"simulation\" + 0.007*\"aspect\"'),\n",
       " (14,\n",
       "  u'0.031*\"robot\" + 0.017*\"policy\" + 0.012*\"action\" + 0.011*\"object\" + 0.009*\"planning\" + 0.009*\"pomdp\" + 0.007*\"belief\" + 0.007*\"of\" + 0.007*\"path\" + 0.006*\"reward\"')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.show_topics(num_topics=num_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_labels = ['Computer Vision', 'Motion Detection', 'Database System', \\\n",
    "                'Clustering', 'Simulation', 'Sampling & Counting', \\\n",
    "                'Gaussian Process', 'Speech Processing', 'NLP', \\\n",
    "                'Reinforcement Learning', 'Recommender System', \\\n",
    "                'Collaborative AI Systems', 'Pose estimation', 'Media Search & Retrieval', \\\n",
    "                'POMDP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: Computer Vision\n",
      "Words: face image facial occlusion age layer object detection deep cnn \n",
      "\n",
      "Label: Motion Detection\n",
      "Words: image fault signal position light motion detection constraint pass robot \n",
      "\n",
      "Label: Database System\n",
      "Words: graph node u gpu transaction query concept memory block record \n",
      "\n",
      "Label: Clustering\n",
      "Words: query image record index cluster attribute similarity edge selection tuple \n",
      "\n",
      "Label: Simulation\n",
      "Words: consumer image goal package simulation concept period product price self \n",
      "\n",
      "Label: Sampling & Counting\n",
      "Words: variable formula count sat q counting constraint benchmark independent rf \n",
      "\n",
      "Label: Gaussian Process\n",
      "Words: gp gaussian agent q u proc in_proc dt z robot \n",
      "\n",
      "Label: Speech Processing\n",
      "Words: speech language pp tool signal alignment acoustic speaker gan deep \n",
      "\n",
      "Label: NLP\n",
      "Words: language translation english rule in_proceeding score sentence relation workshop chinese \n",
      "\n",
      "Label: Reinforcement Learning\n",
      "Words: decision action concept node variable policy transition selection reward relation \n",
      "\n",
      "Label: Recommender System\n",
      "Words: social recommendation temporal activity attribute relation app node entity water \n",
      "\n",
      "Label: Collaborative AI Systems\n",
      "Words: trust kernel cluster pp fig object accuracy vol version gaussian \n",
      "\n",
      "Label: Pose estimation\n",
      "Words: image camera pose of motion and cloud robot vision depth \n",
      "\n",
      "Label: Media Search&Retrieval\n",
      "Words: image item video social ad rating recommendation u simulation aspect \n",
      "\n",
      "Label: POMDP\n",
      "Words: robot policy action object planning pomdp belief of path reward \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for topic in model.show_topics(num_topics=num_topics):\n",
    "    print('Label: ' + topic_labels[topic[0]])\n",
    "    words = ''\n",
    "    for word, prob in model.show_topic(topic[0]):\n",
    "        words += word + ' '\n",
    "    print('Words: ' + words)\n",
    "    print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 0.9478781368818591),\n",
       " (5, 0.01842768025316355),\n",
       " (1, 0.013419519264571128),\n",
       " (6, 0.010673665067926285)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(model['Bingsheng_He'], key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "def show_author(name):\n",
    "    print('\\n%s' % name)\n",
    "    print('Docs:', model.author2doc[name])\n",
    "    print('Topics:')\n",
    "    sorted_topics = sorted(model[name], key=lambda x: x[1], reverse=True)\n",
    "    pprint([(topic_labels[topic[0]], topic[1]) for topic in sorted_topics])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sanjay_Jain\n",
      "('Docs:', [13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24])\n",
      "Topics:\n",
      "[('Simulation', 0.9999642100381495)]\n",
      "\n",
      "Shuicheng_Yan\n",
      "('Docs:', [67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83])\n",
      "Topics:\n",
      "[('Computer Vision', 0.9999779291279178)]\n",
      "\n",
      "Kay_Chen_Tan\n",
      "('Docs:', [105, 158, 159, 108, 160, 115])\n",
      "Topics:\n",
      "[('Speech Processing', 0.515746570543134),\n",
      " ('Collaborative AI Systems', 0.24062687254342172),\n",
      " ('Reinforcement Learning', 0.04410997519241778),\n",
      " ('Computer Vision', 0.03790093564026754),\n",
      " ('Simulation', 0.032284912061083686),\n",
      " ('Recommender System', 0.022506248098537615),\n",
      " ('POMDP', 0.02056526886975451),\n",
      " ('Clustering', 0.019054743514355044),\n",
      " ('Gaussian Process', 0.01877395697699087),\n",
      " ('Motion Detection', 0.018671484159551256),\n",
      " ('Sampling & Counting', 0.016362649905206604)]\n",
      "\n",
      "Tat_Seng_CHUA\n",
      "('Docs:', [173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193])\n",
      "Topics:\n",
      "[('Recommender System', 0.6382006733874321),\n",
      " ('Media Search&Retrieval', 0.36178183987367807)]\n",
      "\n",
      "Bingsheng_He\n",
      "('Docs:', [47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58])\n",
      "Topics:\n",
      "[('Database System', 0.9478781368818591),\n",
      " ('Sampling & Counting', 0.01842768025316355),\n",
      " ('Motion Detection', 0.013419519264571128),\n",
      " ('Gaussian Process', 0.010673665067926285)]\n",
      "\n",
      "Kuldeep_S._Meel\n",
      "('Docs:', [252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273])\n",
      "Topics:\n",
      "[('Sampling & Counting', 0.9999846980631805)]\n",
      "\n",
      "Teck_Khim_Ng\n",
      "('Docs:', [161, 162, 163, 164, 165])\n",
      "Topics:\n",
      "[('Collaborative AI Systems', 0.38507770495361227),\n",
      " ('Computer Vision', 0.2254887420599665),\n",
      " ('Database System', 0.17751431491592526),\n",
      " ('Pose estimation', 0.09868428877279692),\n",
      " ('Reinforcement Learning', 0.03913983473078565),\n",
      " ('Clustering', 0.028562289271339147),\n",
      " ('Simulation', 0.022815854699748894),\n",
      " ('Gaussian Process', 0.010583167505117586)]\n",
      "\n",
      "Wynne_Hsu\n",
      "('Docs:', [274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 174, 286, 287])\n",
      "Topics:\n",
      "[('Recommender System', 0.502026496666443),\n",
      " ('Clustering', 0.270963896635594),\n",
      " ('Media Search&Retrieval', 0.17443215244699814),\n",
      " ('Sampling & Counting', 0.026479216356293758),\n",
      " ('Reinforcement Learning', 0.015306333903886391)]\n",
      "\n",
      "Gim_Hee_Lee\n",
      "('Docs:', [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12])\n",
      "Topics:\n",
      "[('Pose estimation', 0.9240083377105307),\n",
      " ('Computer Vision', 0.0385330115281012),\n",
      " ('POMDP', 0.0226402354214124)]\n",
      "\n",
      "Wee_Sun_Lee\n",
      "('Docs:', [150, 119, 120, 122, 123, 151, 152, 153, 126, 154, 6, 155, 128, 129, 131, 157, 158, 134, 135])\n",
      "Topics:\n",
      "[('POMDP', 0.8663125629724615),\n",
      " ('Gaussian Process', 0.11494292293987429),\n",
      " ('Sampling & Counting', 0.018681843015511897)]\n",
      "\n",
      "Min-Yen_Kan\n",
      "('Docs:', [288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300])\n",
      "Topics:\n",
      "[('Media Search&Retrieval', 0.811879252329091),\n",
      " ('NLP', 0.1305150818401512),\n",
      " ('Clustering', 0.024252795532938583),\n",
      " ('Database System', 0.018035134972576316)]\n",
      "\n",
      "David_Hsu\n",
      "('Docs:', [117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135])\n",
      "Topics:\n",
      "[('POMDP', 0.9973746402359992)]\n",
      "\n",
      "Hwee_Tou_Ng\n",
      "('Docs:', [235, 236, 237, 238, 239, 153, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 158, 250, 251])\n",
      "Topics:\n",
      "[('NLP', 0.9999703615011625)]\n",
      "\n",
      "Mohan_S._Kankanhalli\n",
      "('Docs:', [136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148])\n",
      "Topics:\n",
      "[('Media Search&Retrieval', 0.9473669499914694),\n",
      " ('Computer Vision', 0.04088786716151139)]\n",
      "\n",
      "Anthony_Tung\n",
      "('Docs:', [84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103])\n",
      "Topics:\n",
      "[('Clustering', 0.9541959227491497), ('Database System', 0.045780375359888645)]\n",
      "\n",
      "Lee_Wee_Sun\n",
      "('Docs:', [149, 119, 120, 123, 150, 151, 152, 126, 153, 6, 154, 128, 129, 155, 131, 156, 157, 134, 135])\n",
      "Topics:\n",
      "[('POMDP', 0.9455552250943784),\n",
      " ('Gaussian Process', 0.03784646425259761),\n",
      " ('Motion Detection', 0.016554330845170644)]\n",
      "\n",
      "Rudy_Setiono\n",
      "('Docs:', [59, 60, 61, 62, 63, 64, 65, 66])\n",
      "Topics:\n",
      "[('NLP', 0.3622343538555221),\n",
      " ('Clustering', 0.2727496754963418),\n",
      " ('Reinforcement Learning', 0.13402980679432713),\n",
      " ('Collaborative AI Systems', 0.0849591768388831),\n",
      " ('Simulation', 0.07665754629643283),\n",
      " ('Recommender System', 0.05359554802035544)]\n",
      "\n",
      "David_S._Rosenblum\n",
      "('Docs:', [213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 174, 223])\n",
      "Topics:\n",
      "[('Recommender System', 0.9167810200814753),\n",
      " ('Gaussian Process', 0.026425235098239394),\n",
      " ('Reinforcement Learning', 0.01999317738749726),\n",
      " ('Sampling & Counting', 0.01066176928466601),\n",
      " ('Pose estimation', 0.010519647034944239)]\n",
      "\n",
      "Beng_Chin_OOI\n",
      "('Docs:', [194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 165, 166, 206, 207, 208, 209, 210, 211, 212])\n",
      "Topics:\n",
      "[('Database System', 0.9999830886939179)]\n",
      "\n",
      "Shuzhi_Sam_Ge\n",
      "('Docs:', [166, 167, 168, 60, 169, 170, 171, 172])\n",
      "Topics:\n",
      "[('Motion Detection', 0.9497634758757035),\n",
      " ('Pose estimation', 0.019814923240264064),\n",
      " ('Recommender System', 0.015952811832831357)]\n",
      "\n",
      "Bryan_Low\n",
      "('Docs:', [25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46])\n",
      "Topics:\n",
      "[('Gaussian Process', 0.9998840652486021)]\n",
      "\n",
      "Leong_Tze_Yun\n",
      "('Docs:', [301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311])\n",
      "Topics:\n",
      "[('Reinforcement Learning', 0.821725497879559),\n",
      " ('POMDP', 0.14543660746638176),\n",
      " ('Simulation', 0.032778168011194445)]\n",
      "\n",
      "Harold_Soh\n",
      "('Docs:', [312, 313, 314, 315, 316, 317, 318, 132, 133])\n",
      "Topics:\n",
      "[('Collaborative AI Systems', 0.9680060243326033),\n",
      " ('Gaussian Process', 0.01646178825615212),\n",
      " ('Simulation', 0.01549040956788813)]\n",
      "\n",
      "Terence_Sim\n",
      "('Docs:', [224, 225, 226, 227, 228, 76, 229, 230, 231, 232, 233, 234])\n",
      "Topics:\n",
      "[('Computer Vision', 0.8057642381924203),\n",
      " ('Recommender System', 0.11456229784240557),\n",
      " ('Pose estimation', 0.028391352402054978),\n",
      " ('Speech Processing', 0.026630357383310994)]\n",
      "\n",
      "Haizhou_Li\n",
      "('Docs:', [104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116])\n",
      "Topics:\n",
      "[('Speech Processing', 0.9274908375001015), ('NLP', 0.07245146160141244)]\n"
     ]
    }
   ],
   "source": [
    "for author in author2doc.keys():\n",
    "    show_author(author)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import atmodel\n",
    "doc2author = atmodel.construct_doc2author(model.corpus, model.author2doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the per-word bound.\n",
    "# Number of words in corpus.\n",
    "corpus_words = sum(cnt for document in model.corpus for _, cnt in document)\n",
    "\n",
    "# Compute bound and divide by number of words.\n",
    "perwordbound = model.bound(model.corpus, author2doc=model.author2doc, \\\n",
    "                           doc2author=model.doc2author) / corpus_words\n",
    "print(perwordbound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time top_topics = model.top_topics(model.corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(n_components=2, random_state=0)\n",
    "smallest_author = 0  # Ignore authors with documents less than this.\n",
    "authors = [model.author2id[a] for a in model.author2id.keys() if len(model.author2doc[a]) >= smallest_author]\n",
    "_ = tsne.fit_transform(model.state.gamma[authors, :])  # Result stored in tsne.embedding_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tell Bokeh to display plots inside the notebook.\n",
    "from bokeh.io import output_notebook\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.models import HoverTool\n",
    "from bokeh.plotting import figure, show, ColumnDataSource\n",
    "\n",
    "x = tsne.embedding_[:, 0]\n",
    "y = tsne.embedding_[:, 1]\n",
    "author_names = [model.id2author[a] for a in authors]\n",
    "\n",
    "# Radius of each point corresponds to the number of documents attributed to that author.\n",
    "scale = 0.1\n",
    "author_sizes = [len(model.author2doc[a]) for a in author_names]\n",
    "radii = [size * scale for size in author_sizes]\n",
    "\n",
    "source = ColumnDataSource(\n",
    "        data=dict(\n",
    "            x=x,\n",
    "            y=y,\n",
    "            author_names=author_names,\n",
    "            author_sizes=author_sizes,\n",
    "            radii=radii,\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Add author names and sizes to mouse-over info.\n",
    "hover = HoverTool(\n",
    "        tooltips=[\n",
    "        (\"author\", \"@author_names\"),\n",
    "        (\"size\", \"@author_sizes\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "p = figure(tools=[hover, 'crosshair,pan,wheel_zoom,box_zoom,reset,save,lasso_select'])\n",
    "p.scatter('x', 'y', radius='radii', source=source, fill_alpha=0.6, line_color=None)\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.similarities import MatrixSimilarity\n",
    "\n",
    "# Generate a similarity object for the transformed corpus.\n",
    "index = MatrixSimilarity(model[list(model.id2author.values())])\n",
    "\n",
    "# Get similarities to some author.\n",
    "author_name = 'Bryan_Low'\n",
    "sims = index[model[author_name]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a function that returns similarities based on the Hellinger distance.\n",
    "\n",
    "from gensim import matutils\n",
    "import pandas as pd\n",
    "\n",
    "# Make a list of all the author-topic distributions.\n",
    "author_vecs = [model.get_author_topics(author) for author in model.id2author.values()]\n",
    "\n",
    "def similarity(vec1, vec2):\n",
    "    '''Get similarity between two vectors'''\n",
    "    dist = matutils.hellinger(matutils.sparse2full(vec1, model.num_topics), \\\n",
    "                              matutils.sparse2full(vec2, model.num_topics))\n",
    "    sim = 1.0 / (1.0 + dist)\n",
    "    return sim\n",
    "\n",
    "def get_sims(vec):\n",
    "    '''Get similarity of vector to all authors.'''\n",
    "    sims = [similarity(vec, vec2) for vec2 in author_vecs]\n",
    "    return sims\n",
    "\n",
    "def get_table(name, top_n=10, smallest_author=1):\n",
    "    '''\n",
    "    Get table with similarities, author names, and author sizes.\n",
    "    Return `top_n` authors as a dataframe.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Get similarities.\n",
    "    sims = get_sims(model.get_author_topics(name))\n",
    "\n",
    "    # Arrange author names, similarities, and author sizes in a list of tuples.\n",
    "    table = []\n",
    "    for elem in enumerate(sims):\n",
    "        author_name = model.id2author[elem[0]]\n",
    "        sim = elem[1]\n",
    "        author_size = len(model.author2doc[author_name])\n",
    "        if author_size >= smallest_author:\n",
    "            table.append((author_name, sim, author_size))\n",
    "            \n",
    "    # Make dataframe and retrieve top authors.\n",
    "    df = pd.DataFrame(table, columns=['Author', 'Score', 'Size'])\n",
    "    df = df.sort_values('Score', ascending=False)[:top_n]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author</th>\n",
       "      <th>Score</th>\n",
       "      <th>Size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bryan_Low</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Wee_Sun_Lee</td>\n",
       "      <td>0.551585</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Lee_Wee_Sun</td>\n",
       "      <td>0.527027</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>David_S._Rosenblum</td>\n",
       "      <td>0.523337</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Kay_Chen_Tan</td>\n",
       "      <td>0.519392</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Harold_Soh</td>\n",
       "      <td>0.517168</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Teck_Khim_Ng</td>\n",
       "      <td>0.514421</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bingsheng_He</td>\n",
       "      <td>0.514305</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Terence_Sim</td>\n",
       "      <td>0.501558</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Rudy_Setiono</td>\n",
       "      <td>0.500997</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Author     Score  Size\n",
       "3            Bryan_Low  1.000000    22\n",
       "23         Wee_Sun_Lee  0.551585    19\n",
       "12         Lee_Wee_Sun  0.527027    19\n",
       "5   David_S._Rosenblum  0.523337    12\n",
       "10        Kay_Chen_Tan  0.519392     6\n",
       "8           Harold_Soh  0.517168     9\n",
       "21        Teck_Khim_Ng  0.514421     5\n",
       "2         Bingsheng_He  0.514305    12\n",
       "22         Terence_Sim  0.501558    12\n",
       "16        Rudy_Setiono  0.500997     8"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_table('Bryan_Low')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
